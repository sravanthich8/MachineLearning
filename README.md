# MachineLearning
Book Recommendation System using Collaborative Filtering
The Dataset contains three .csv files with Books Rating, User details and Books in general. We have a total of 271K records on books, and the total number of registered users on the website is over 276K, and Book Ratings file with 10 lakh records. This makes it a huge dataset to analyze. Before Coding and deploying the Book Recommendation engine, an important task in our hand is to understand, explore and preprocess the dataset for further analysis. The Dataset is divided into smaller 3 datasets with many attributes, needs to be explored and doing this is vital to understand the characteristics and dimensions of the dataset before training and testing the model. 
As a first step, we load the data from .csv files to Jupyter notebook. The Data contains encoded information for which we have used the Encoding as latin-1 while importing the data to fix the Encoding errors. Next, we have performed a series Data Cleaning steps and Exploratory Data Analysis on the three Files which includes Column renaming, fill NA values and also looked at the demographics of Age, Ratings and Year of Publication column. To visualize information more clearly, we plotted bar chart and pulled data for top ten authors with most books’ vs total number of books. From this graph we got to know that Agatha Christie author is the first among all with 632 book publications
Having a look at the distribution of outliers in age and the missing number of values we came to the point that we cannot afford to just drop off these Nan and outliers because even these users have given ratings and we have User-Item interactions for these users. Instead, we first imputed the outliers with “Nan” and filled the Null values with Range of Age grouped by Country and with Median value. 
For Year of Publication column in the Books Dataset, we observed that the year mentioned was beyond 2021 for some entries. We first filled them with Nan values. Then we also observed that the Year of Publication graph was positive skewed, so we imputed the Nan values with the median. Ratings with value 0 cannot give us the right prediction for which we have removed values which showed rating as 0.  
We separated rating from 1 to 10 and created two new columns rating average and rating sum. Since the data is now free of missing data and that we have eliminated extreme values. It can be observed that higher ratings are more common amongst users and rating 8 has been rated the highest number of times. Now that we have a clean dataset and analyzed our data better, we can now proceed with our model application.
Modeling Approach:
There are 2 major approaches for building recommendation systems — content-based and collaborative filtering. In this Project, we have looked at different Collaborative Modelling Techniques, explained how we implemented them to achieve our goal. 
Collaborative Filtering:
If a Recommender system suggests items to a user based on past interactions between users and items, that system is known as a Collaborative Filtering system. In these recommendation engines, a user-item interactions matrix is created such that every user and item pair have a space in the matrix. That space is either filled with the user's rating of that item or it is left blank. This can be used for matrix factorization or nearest neighbor classification, both of which will be addressed when we develop our models. The important thing to remember with collaborative filtering is that user id, item id, and rating are the only fields required. Collaborative models can be user-based or item-based. After EDA, we implemented five models for our Book Recommendation System.
Popularity Based Approach:
As a Base Model, we created the recommendation system based on Popularity of the books. Here we defined a function named as “Weighted_Ratings” where we calculated the weight according to a specific formula “(v/(v+m) * R) + (m/(m+v) * C)” where ‘v’ is the count of ratings, ‘R’ is the rating average, ‘m’ is the quantile, and ‘C’ is the mean. Then based on the score, we sorted the data as shown below
Collaborative Filtering:
KNN model with Cosine Similarity:
Then we applied the Collaborative Filtering Model where we considered only the relevant features like ISBN, book-ratings, rating_avg, rating_sum, etc. We also considered only the ratings whose count was greater than 50. 
Then we applied the KNN (K Nearest Neighbours) model where we used metric = Cosine and algorithm = brute as this combination worked best for our model. Then we calculated distances and indices. The book with the least distance is most likely to be the recommended book as compared to the book with greater distance.
Kmeans Clustering with PCA:
Furthermore, we decided to implement KMeans with PCA as our next model for location based recommendations. Before applying the model, we performed some calculation and updated the data. For example we created a binary column “is_fav” which depends on user_rating and mean_rating. In the example below we can see that if the Book_Rating is greater than the mean_rating, then the “is_fav” column is set to 1 and vice-versa. Then we created a matrix for user and item and applied PCA for dimensionality reduction.
We brought our multi-dimensional dataset to 3-D by using PCA. For choosing the K value we used Elbow method at first, but we did not get a clear elbow. So, we worked on Sihouette Analysis and found that K=4 provides the best Clustering.
Model Based Approach using Surprise Package (KNN Baseline, KNN Means, Baseline models):
The Surprise package in Python is newer but provided all the tools we needed to test out multiple algorithms for Collaborative Filtering and then guided us through tuning the parameters and cross validating to determine the optimal model. In order to this, We chose three versions of the data to analyze. First, We used the full data to include books that have as little as 1 book rating and 1 user rating. We then created a list of midlist books by filtering down to 5 book rating and 20 user ratings. Finally, we used the midlist books by filtering down to 10 book rating and 30 user ratings. We have results for multiple algorithms to see which would be best for this data. we can see that the Best RMSE was provided by the BaselineOnly algorithm
After completing the grid searches, we ran 10-fold cross validation on each of the tuned models and applied the best parameters for the optimized models. We observe that the least RMSE’s of KNN with Means, KNN Baseline and BaselineOnly are 1.7731, 1.5455 and 1.4686 respectively.
We have trained the optimized algorithm on the dataset and produced the recommended books for specific user. Above we can observe an example for a random user. Initially we have predicted a list of 10 Books for each User, and then we have considered one single user and predicted the ratings that he would rate his book suggestions. 
Majority of the readers were of the age bracket 20–35 and most of them came from North American and European countries namely USA, Canada, UK, Germany, and Spain. If we look at the ratings distribution, most of the books have high ratings with maximum books being rated 8. Ratings below 5 are few. Author with the most books was Agatha Christie, William Shakespeare, and Stephen King. If we look at the popularity-based approach, we could generate total users count along with their rating for most popular books. It was observed that for collaborative filtering approach, we were able to populate top 5 book recommendations based on index number generated. For KMeans with PCA, we performed KMeans Clustering, Elbow method, Silhouette Analysis to predict top 5 books and top 5 authors using clusters. K-Means clustering allowed us to draw conclusions and even design a useful application around it
Based on Surprise results, KNNBaseline ultimately performed better compared to BaselineOnly, KNNWithMeans. Using KNNBaseline Approach, we were able to recommend books for any user.
